dependencies:
 'lib.crawl': 'crawl'

extract:
  # Use this step to crawl the first page of company listings
  - step: 1A. Download the page with the first 10 company links
    action: extract.source.http.DownloadFiles
    allow_redirects: True
    fetch:
      - 'http://data-interview.enigmalabs.org/companies'

  # Use the next two steps for multipage crawl using custom step
  # - step: 1B-1. Call custom step to get the URL for each index page
  #   action: crawl.CrawlPages
  #   base_url: http://data-interview.enigmalabs.org/companies

  # - step: 1B-2. Download each index page
  #   action: extract.source.http.DownloadFiles

  - step: Extract company endpoints from index page
    action: extract.formats.html.ExtractLink
    criteria:
      id: '^\d+$'

  - step: Download each company endpoint
    action: extract.source.http.DownloadFiles

transform:
  - step: Set the company listings table schema
    action: transform.schema.SetSchema
    fields:
      - name: Company_Name
        type: string
      - name: Company_Website
        type: string
      - name: Company_Description
        type: string

  - step: Parse the company listing HTML
    action: extract.formats.html.ReadRecords
    record_selector: tbody
    extract:
      - selector: tr > td
        child_index: 1
        property: text
      - selector: tr > td
        child_index: 15
        property: text
      - selector: td
        child_index: 17
        property: text

  - step: Set file name
    action: load.SetTableName
    table_name: company_listings

  - step: Save locally
    action: load.OutputCSV
    repository: .
