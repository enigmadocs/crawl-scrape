dependencies:
 'lib.crawl': 'crawl'

extract:
  # Use this step to crawl just the first page of company listings
  # - step: Download the page with the first 10 company links
  #   action: extract.source.http.DownloadFiles
  #   allow_redirects: True
  #   fetch:
  #     - 'http://data-interview.enigmalabs.org/companies'

  # Use this step for multipage crawl using custom step
  - step: Call custom step to download each of the index pages
    action: crawl.CrawlPages
    base_url: http://data-interview.enigmalabs.org/companies

  - step: Extract company endpoints from index page
    action: extract.formats.html.ExtractLink
    criteria:
      id: '^\d+$'

  - step: Download each company endpoint
    action: extract.source.http.DownloadFiles

transform:
  - step: Set the company listings table schema
    action: transform.schema.SetSchema
    fields:
      - name: Company_Name
        type: string
      - name: Company_Website
        type: string
      - name: Company_Description
        type: string

  - step: Parse the company listing HTML
    action: extract.formats.html.ReadRecords
    record_selector: tbody
    extract:
      - selector: tr > td
        child_index: 1
        property: text
      - selector: tr > td
        child_index: 15
        property: text
      - selector: td
        child_index: 17
        property: text

  - step: Set file name
    action: load.SetTableName
    table_name: company_listings

  - step: Save locally
    action: load.OutputCSV
    repository: .
